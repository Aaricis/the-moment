import json
import os
import sys

sys.path.append(os.path.split(sys.path[0])[0])  # æ·»åŠ åŒ…æœç´¢è·¯å¾„
import textwrap

import gradio as gr
import requests
import torch
from dotenv import load_dotenv
from gradio import ChatMessage
from loguru import logger
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

from src.configs.tts_config import audio_path, slicer_list
from src.utils.loader import load_text_audio_mappings

from generate import async_chat
from src.configs.base_config import model_path
from src.configs.rag_config import prompt_template
from src.rag.pipeline import EmoLLMRAG

load_dotenv()  # è‡ªåŠ¨æŠŠ .env è¯»å…¥ç¯å¢ƒå˜é‡

LANGSEARCH_API_URL = "https://api.langsearch.com/v1/web-search"
LANGSEARCH_API_KEY = os.getenv('LANGSEARCH_API_KEY')


def lang_search(query, max_results=5):
    """
    è”ç½‘æœç´¢
    :param query: ç”¨æˆ·é—®é¢˜
    :param max_results: è¿”å›ç»“æœæ•°é‡
    :return: æœç´¢ç»“æœ
    """
    payload = json.dumps({
        "query": query,
        "freshness": "noLimit",
        "summary": True,
        "count": max_results
    })

    headers = {
        "Authorization": f"Bearer {LANGSEARCH_API_KEY}",
        "Content-Type": "application/json"
    }

    response = requests.post(LANGSEARCH_API_URL, headers=headers, data=payload)
    if response.status_code == 200:
        logger.info("Response Success: 200")
        results = json.loads(response.text).get("data").get("webPages").get("value")
        search_results = []
        for result in results:
            title = result.get("name", "")
            snippet = result.get("snippet", "")
            url = result.get("url", "")
            search_results.append(f"æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{snippet}\né“¾æ¥ï¼š{url}\n")
        return "\n".join(search_results)
    else:
        logger.error(f"Response Error: {response.status_code}")
        return ""


@torch.inference_mode()
async def generate_response_and_tts(
        history,
        temperature,
        top_p,
        max_new_tokens,
        repetition_penalty,
        active_gen,
        selected_text,
        ref_text,
        prompt_language,
        text_language,
        how_to_cut
):
    print("history len =", len(history), "history =", history)
    print(f"type = {type(history)}")

    # è·å–ç”¨æˆ·æ¶ˆæ¯ï¼ˆå€’æ•°ç¬¬äºŒæ¡ï¼‰
    user_message = history[-2]["content"]
    print(f"user_message = {user_message}")

    conversation = []
    # éå†é™¤æœ€åä¸¤æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    for msg in history[:-2]:
        if msg.get("content") and msg.get("content").strip():  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            conversation.append({
                "role": msg.get("role", "user"),  # é»˜è®¤è§’è‰²ä¸ºuser
                "content": msg.get("content", "")
            })

    # è”ç½‘æœç´¢
    searched_results = lang_search(user_message)
    if searched_results:
        logger.info(f"è”ç½‘æœç´¢ç»“æœï¼š{searched_results}")
    else:
        logger.info("è”ç½‘æœªæœç´¢åˆ°å‡†ç¡®ä¿¡æ¯")

    # çŸ¥è¯†åº“æœç´¢
    rag = EmoLLMRAG()
    retrieved_context = rag.get_retrieval_content(user_message)
    if retrieved_context:
        logger.info(f"çŸ¥è¯†åº“æœç´¢ç»“æœï¼š{retrieved_context}")
    else:
        logger.info("çŸ¥è¯†åº“æœªæœç´¢åˆ°å‡†ç¡®ä¿¡æ¯")

    conversation.append(
        {
            "role": "user",
            "content": textwrap.dedent(prompt_template).format(
                user_input=user_message,
                searched_results=searched_results,
                retrieved_context=retrieved_context
            )
        }
    )

    input_ids = tokenizer.apply_chat_template(
        conversation,
        tokenize=True,  # ç›´æ¥è¾“å‡º token
        add_generation_prompt=True,  # åœ¨ç»“å°¾åŠ ä¸Šâ€œAssistant å¼€å§‹å›ç­”â€çš„æç¤ºï¼Œè®©æ¨¡å‹è¿›å…¥ç”Ÿæˆæ¨¡å¼
        return_tensors="pt"  # ç›´æ¥è¿”å› PyTorch tensor
    ).to(model.device)

    streamer = TextIteratorStreamer(  # æµå¼è¾“å‡º
        tokenizer,
        timeout=20.0,
        skip_prompt=True,  # ä¸è¿”å› prompt æ–‡æœ¬ï¼ˆåªè¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        skip_special_tokens=True  # æ»¤æ‰ç‰¹æ®Šç¬¦å·<endoftext>, <pad>
    )

    generate_kwargs = dict(
        input_ids=input_ids,
        streamer=streamer,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        do_sample=True,
        max_new_tokens=max_new_tokens,
        tokenizer=tokenizer,
    )

    async for history, audio, conversion_time in async_chat(
            active_gen,
            history,
            tokenizer,
            model,
            generate_kwargs,
            selected_text,
            ref_text,
            prompt_language,
            text_language,
            how_to_cut
    ):
        yield history, audio, conversion_time


# åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥å¤„ç†æµå¼è¾“å‡º
async def generate_wrapper(
        chatbot,
        temperature,
        top_p,
        max_new_tokens,
        repetition_penalty,
        active_gen,
        selected_text,
        ref_text,
        prompt_language,
        text_language,
        how_to_cut
):
    final_chatbot = None
    final_audio = None
    final_tts_time = None

    async for chatbot, audio, tts_time in generate_response_and_tts(
            chatbot,
            temperature,
            top_p,
            max_new_tokens,
            repetition_penalty,
            active_gen,
            selected_text,
            ref_text,
            prompt_language,
            text_language,
            how_to_cut
    ):
        final_chatbot = chatbot
        final_audio = audio
        final_tts_time = tts_time
        # å¦‚æœæ˜¯ä¸­é—´ç»“æœï¼Œåªæ›´æ–°chatbot
        if audio is None:
            yield chatbot, None, None

    # è¿”å›æœ€ç»ˆç»“æœ
    yield final_chatbot, final_audio, final_tts_time


def build_app():
    css = """
    .spinner {
        animation: spin 1s linear infinite;
        display: inline-block;
        margin-right: 8px;
    }
    @keyframes spin {
        from { transform: rotate(0deg); }
        to { transform: rotate(360deg); }
    }
    .thinking-summary {
        cursor: pointer;
        padding: 8px;
        background: #f5f5f5;
        border-radius: 4px;
        margin: 4px 0;
    }
    .thought-content {
        padding: 10px;
        background: #f8f9fa;
        border-radius: 4px;
        margin: 5px 0;
    }
    .thinking-container {
        border-left: 3px solid #facc15;
        padding-left: 10px;
        margin: 8px 0;
        background: #ffffff;
    }
    details:not([open]) .thinking-container {
        border-left-color: #290c15;
    }
    details {
        border: 1px solid #e0e0e0 !important;
        border-radius: 8px !important;
        padding: 12px !important;
        margin: 8px 0 !important;
        transition: border-color 0.2s;
    }
    """

    description = '''
    # ğŸ§  An AI assistant with extensive knowledge in psychology, and my name is Care.

    ## ğŸš€ Overview
    This model is finetuned on deepseek-r1. If this repo helps you, star and share it â¤ï¸. This repo will be continuously merged into EmoLLM.

    ## âœ¨ Functions
    âœ…Provide an interactive chat interface for psychological consultation seekers.

    âœ…Integrate knowledge retrieval

    âœ…Integrate web searching

    âœ…Two customized tts (ISSUE: more voice models)

    âœ…Display the consuming time of generating voice with the streaming way

    âŒVirtual mental companion

    ## âš ï¸ issue status
    - 2025.4.29 fix bug of clearing and stopping op.
    - 2025.5.3 web search supports.
    - 2025.5.5 rag supports. (demo code, needs to be checked)
    - 2025.5.7 fix bug of rag.
    - 2025.5.9 tts supports.
    - 2025.5.10 two voice models.
    - 2025.5.16 merge into EmoLLM.
    - 2025.8.22 display the time of streaming voice.

    ## ğŸ™ Acknowledgments
    We are grateful to Modelscope for supporting this project with resources.

    The rag codes are based on [EmoLLM](https://github.com/SmartFlowAI/EmoLLM )

    ## ğŸ¤ Contributing
    Feel free to contribute to this project via our [github repo](https://github.com/HaiyangPeng/careyou ). Grow together!
    '''

    def user(message, history):
        if not message:
            return "", history
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})  # ç©ºå­—ç¬¦ä¸²å ä½
        return "", history

    with gr.Blocks(css=css) as demo:
        gr.Markdown(description)
        active_gen = gr.State([False])

        chatbot = gr.Chatbot(
            elem_id="chatbot",
            height=500,
            show_label=False,
            render_markdown=True,
            type="messages"
        )

        with gr.Row():
            msg = gr.Textbox(
                label="Message",
                placeholder="Type your message...",
                container=False,
                scale=4
            )
            submit_btn = gr.Button("Send", variant='primary', scale=1)

        with gr.Row():
            clear_btn = gr.Button("Clear", variant='secondary')
            stop_btn = gr.Button("Stop", variant='stop')
        with gr.Accordion("Parameters", open=False):
            temperature = gr.Slider(
                minimum=0.1,
                maximum=1.5,
                value=0.6,
                label="Temperature"
            )

            top_p = gr.Slider(
                minimum=0.1,
                maximum=1.0,
                value=0.95,
                label="Top-p"
            )

            max_new_tokens = gr.Slider(
                minimum=2048,
                maximum=32768,
                value=4096,
                step=64,
                label="Max Tokens"
            )

            repetition_penalty = gr.Slider(
                minimum=1,
                maximum=1.5,
                value=1.2,
                step=0.01,
                label="Repetition Penalty"
            )

        gr.Examples(
            examples=[
                ["ä½ æ˜¯è°å‘€"],
                ["æˆ‘å¾ˆéš¾è¿‡ï¼Œçˆ¸å¦ˆä¸çˆ±æˆ‘"],
                ["çˆ¸å¦ˆè€æ˜¯è¯´æˆ‘ç¬¨"]
            ],
            inputs=msg,  # ç‚¹å‡»ç¤ºä¾‹åï¼Œè‡ªåŠ¨å¡«å……åˆ°å‰é¢çš„ msg æ–‡æœ¬æ¡†
            label="å’¨è¯¢ä¾‹å­"
        )

        output_audio = gr.Audio(label="converted voice", streaming=True, autoplay=True)
        tts_time_display = gr.Textbox(label="TTS Conversion Time", value="0s", interactive=False)  # åªè¯»ï¼Œç”¨æˆ·ä¸èƒ½ä¿®æ”¹

        text_to_audio_mappings = load_text_audio_mappings(audio_path, slicer_list)
        default_audio_select = list(text_to_audio_mappings.keys())[0] if text_to_audio_mappings else ""
        default_ref_text = default_audio_select
        default_prompt_language = "zh"
        default_text_language = "zh"
        default_how_to_cut = "ä¸åˆ‡"

        # ä½¿ç”¨åŒ…è£…å‡½æ•°
        submit_event = submit_btn.click(
            user, [msg, chatbot], [msg, chatbot], queue=False
        ).then(
            lambda: [True], outputs=active_gen
        ).then(
            generate_wrapper,
            [
                chatbot,
                temperature,
                top_p,
                max_new_tokens,
                repetition_penalty,
                active_gen,
                gr.State(default_audio_select),
                gr.State(default_ref_text),
                gr.State(default_prompt_language),
                gr.State(default_text_language),
                gr.State(default_how_to_cut)
            ],
            [
                chatbot,
                output_audio,
                tts_time_display
            ]
        )

        stop_btn.click(
            lambda: [False], None, active_gen, cancels=[submit_event]
        )

        clear_btn.click(
            lambda: (None, None, "0s"), None, [chatbot, output_audio, tts_time_display], queue=False
        ).then(
            lambda: [False], None, active_gen, cancels=[submit_event]
        )

        stop_btn.click(
            lambda: [False], None, active_gen, cancels=[submit_event]
        )

        clear_btn.click(
            lambda: (None, None, "0s"), None, [chatbot, output_audio, tts_time_display], queue=False
        ).then(
            lambda: [False], None, active_gen, cancels=[submit_event]
        )

    return demo


if __name__ == "__main__":
    logger.info("Loading Deepseek-R1 model...")

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer.use_default_system_prompt = False  # å…³é—­ tokenizer è‡ªåŠ¨åœ¨å¯¹è¯æœ€å‰é¢è¿½åŠ ã€Œç³»ç»Ÿé»˜è®¤æç¤ºè¯ã€çš„è¡Œä¸º

    app = build_app()
    app.queue(api_open=True, max_size=20, default_concurrency_limit=20).launch(server_name="0.0.0.0", server_port=7860,
                                                                               max_threads=40)
