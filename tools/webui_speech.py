import json
import os
import sys

sys.path.append(os.path.split(sys.path[0])[0])  # æ·»åŠ åŒ…æœç´¢è·¯å¾„
import textwrap

import gradio as gr
import requests
import torch
from dotenv import load_dotenv
from loguru import logger
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer

from src.configs.tts_config import audio_path, slicer_list
from src.utils.loader import load_text_audio_mappings

from generate import async_chat
from src.configs.base_config import model_path
from src.configs.rag_config import prompt_template
from src.rag.pipeline import EmoLLMRAG
import base64
from pathlib import Path

load_dotenv()  # è‡ªåŠ¨æŠŠ .env è¯»å…¥ç¯å¢ƒå˜é‡

LANGSEARCH_API_URL = "https://api.langsearch.com/v1/web-search"
LANGSEARCH_API_KEY = os.getenv('LANGSEARCH_API_KEY')


def lang_search(query, max_results=5):
    """
    è”ç½‘æœç´¢
    :param query: ç”¨æˆ·é—®é¢˜
    :param max_results: è¿”å›ç»“æœæ•°é‡
    :return: æœç´¢ç»“æœ
    """
    payload = json.dumps({
        "query": query,
        "freshness": "noLimit",
        "summary": True,
        "count": max_results
    })

    headers = {
        "Authorization": f"Bearer {LANGSEARCH_API_KEY}",
        "Content-Type": "application/json"
    }

    response = requests.post(LANGSEARCH_API_URL, headers=headers, data=payload)
    if response.status_code == 200:
        logger.info("Response Success: 200")
        results = json.loads(response.text).get("data").get("webPages").get("value")
        search_results = []
        for result in results:
            title = result.get("name", "")
            snippet = result.get("snippet", "")
            url = result.get("url", "")
            search_results.append(f"æ ‡é¢˜ï¼š{title}\næ‘˜è¦ï¼š{snippet}\né“¾æ¥ï¼š{url}\n")
        return "\n".join(search_results)
    else:
        logger.error(f"Response Error: {response.status_code}")
        return ""


@torch.inference_mode()
async def generate_response_and_tts(
        history,
        temperature,
        top_p,
        max_new_tokens,
        repetition_penalty,
        active_gen,
        selected_text,
        ref_text,
        prompt_language,
        text_language,
        how_to_cut
):
    print("history len =", len(history), "history =", history)
    print(f"type = {type(history)}")

    # è·å–ç”¨æˆ·æ¶ˆæ¯ï¼ˆå€’æ•°ç¬¬äºŒæ¡ï¼‰
    user_message = history[-2]["content"]
    print(f"user_message = {user_message}")

    conversation = []
    # éå†é™¤æœ€åä¸¤æ¡ä¹‹å¤–çš„æ‰€æœ‰æ¶ˆæ¯
    for msg in history[:-2]:
        if msg.get("content") and msg.get("content").strip():  # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º
            conversation.append({
                "role": msg.get("role", "user"),  # é»˜è®¤è§’è‰²ä¸ºuser
                "content": msg.get("content", "")
            })

    # è”ç½‘æœç´¢
    searched_results = lang_search(user_message)
    if searched_results:
        logger.info(f"è”ç½‘æœç´¢ç»“æœï¼š{searched_results}")
    else:
        logger.info("è”ç½‘æœªæœç´¢åˆ°å‡†ç¡®ä¿¡æ¯")

    # çŸ¥è¯†åº“æœç´¢
    rag = EmoLLMRAG()
    retrieved_context = rag.get_retrieval_content(user_message)
    if retrieved_context:
        logger.info(f"çŸ¥è¯†åº“æœç´¢ç»“æœï¼š{retrieved_context}")
    else:
        logger.info("çŸ¥è¯†åº“æœªæœç´¢åˆ°å‡†ç¡®ä¿¡æ¯")

    conversation.append(
        {
            "role": "user",
            "content": textwrap.dedent(prompt_template).format(
                user_input=user_message,
                searched_results=searched_results,
                retrieved_context=retrieved_context
            )
        }
    )

    input_ids = tokenizer.apply_chat_template(
        conversation,
        tokenize=True,  # ç›´æ¥è¾“å‡º token
        add_generation_prompt=True,  # åœ¨ç»“å°¾åŠ ä¸Šâ€œAssistant å¼€å§‹å›ç­”â€çš„æç¤ºï¼Œè®©æ¨¡å‹è¿›å…¥ç”Ÿæˆæ¨¡å¼
        return_tensors="pt"  # ç›´æ¥è¿”å› PyTorch tensor
    ).to(model.device)

    streamer = TextIteratorStreamer(  # æµå¼è¾“å‡º
        tokenizer,
        timeout=20.0,
        skip_prompt=True,  # ä¸è¿”å› prompt æ–‡æœ¬ï¼ˆåªè¾“å‡ºæ¨¡å‹ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
        skip_special_tokens=True  # æ»¤æ‰ç‰¹æ®Šç¬¦å·<endoftext>, <pad>
    )

    generate_kwargs = dict(
        input_ids=input_ids,
        streamer=streamer,
        temperature=temperature,
        top_p=top_p,
        repetition_penalty=repetition_penalty,
        do_sample=True,
        max_new_tokens=max_new_tokens,
        tokenizer=tokenizer,
    )

    async for history, audio, conversion_time in async_chat(
            active_gen,
            history,
            tokenizer,
            model,
            generate_kwargs,
            selected_text,
            ref_text,
            prompt_language,
            text_language,
            how_to_cut
    ):
        yield history, audio, conversion_time


# åˆ›å»ºä¸€ä¸ªåŒ…è£…å‡½æ•°æ¥å¤„ç†æµå¼è¾“å‡º
async def generate_wrapper(
        chatbot,
        temperature,
        top_p,
        max_new_tokens,
        repetition_penalty,
        active_gen,
        selected_text,
        ref_text,
        prompt_language,
        text_language,
        how_to_cut
):
    async for chatbot, audio, tts_time in generate_response_and_tts(
            chatbot,
            temperature,
            top_p,
            max_new_tokens,
            repetition_penalty,
            active_gen,
            selected_text,
            ref_text,
            prompt_language,
            text_language,
            how_to_cut
    ):

        # å¦‚æœæ˜¯ä¸­é—´ç»“æœï¼Œåªæ›´æ–°chatbot
        if audio is None:
            # æ–‡æœ¬æµ
            yield chatbot, None, None
        else:
            # éŸ³é¢‘æµ
            yield chatbot, audio, tts_time



import gradio as gr
import base64
from pathlib import Path
import whisper
import os
import soundfile as sf

# ========== åŠ è½½ Whisper æ¨¡å‹ï¼ˆåªåŠ è½½ä¸€æ¬¡ï¼‰ ==========
whisper_model = whisper.load_model("small")


def whisper_recognize(audio):
    """å°†éº¦å…‹é£å½•éŸ³è½¬æ¢ä¸ºä¸­æ–‡æ–‡æœ¬"""
    if audio is None:
        return ""
    sr, data = audio
    tmp_path = "temp_record.wav"
    sf.write(tmp_path, data, sr)
    result = whisper_model.transcribe(
        tmp_path,
        language="zh",
        initial_prompt="ä»¥ä¸‹ä¸ºä¸­æ–‡è¯­éŸ³ï¼Œè¯·ä¿ç•™æ ‡ç‚¹ç¬¦å·ã€‚"
    )
    os.remove(tmp_path)
    return result["text"]


def build_app():
    assets_dir = Path(__file__).parent.parent / "assets"
    bg_path = str(assets_dir / "bg.jpg")

    # è½¬ base64
    with open(bg_path, "rb") as f:
        bg_base64 = base64.b64encode(f.read()).decode()

    css = f"""
    html, body {{
        height: 100%;
        margin: 0 !important;
        padding: 0 !important;
        background: linear-gradient(rgba(18,18,18,0.65), rgba(18,18,18,0.65)),
                    url("data:image/jpg;base64,{bg_base64}") no-repeat center center fixed !important;
        background-size: cover !important;
        font-family: 'Inter', 'Segoe UI', sans-serif;
        color: #e5e7eb;
        overflow-x: hidden;
        animation: fadeIn 1s ease-out;
    }}
    @keyframes fadeIn {{
        from {{ opacity: 0; transform: translateY(10px); }}
        to {{ opacity: 1; transform: translateY(0); }}
    }}
    .container {{
        backdrop-filter: blur(18px);
        -webkit-backdrop-filter: blur(18px);
        background: rgba(255,255,255,0.08);
        border-radius: 24px;
        border: 1px solid rgba(255,255,255,0.18);
        box-shadow: 0 8px 40px rgba(0,0,0,0.45);
        padding: 32px;
        margin: 50px auto;
        max-width: 900px;
    }}
    .app-title {{
        font-size: 2.2rem;
        font-weight: 700;
        text-align: center;
        color: #facc15;
        text-shadow: 0 0 12px rgba(250,204,21,0.6);
        margin-bottom: 20px;
    }}
    #chatbot {{
        background: rgba(255,255,255,0.05);
        border-radius: 18px;
        border: 1px solid rgba(255,255,255,0.18);
        padding: 16px;
        height: 500px !important;
        overflow-y: auto !important;
        box-shadow: inset 0 0 20px rgba(0,0,0,0.3);
    }}
    """

    # --- æ¶ˆæ¯å…¥åˆ—é€»è¾‘ ---
    def user(message, history):
        if not message:
            return "", history
        history.append({"role": "user", "content": message})
        history.append({"role": "assistant", "content": ""})
        return "", history

    with gr.Blocks(css=css, title="The Moment") as demo:
        with gr.Column(elem_classes="container"):
            gr.HTML("<div class='app-title'>âœ¨ The Moment âœ¨</div>")

            active_gen = gr.State([False])
            chatbot = gr.Chatbot(
                elem_id="chatbot",
                height=500,
                show_label=False,
                render_markdown=True,
                type="messages",
                show_copy_button=True
            )

            # ===== è¾“å…¥åŒºï¼ˆæ–‡å­— + è¯­éŸ³ï¼‰ =====
            with gr.Row():
                msg = gr.Textbox(
                    placeholder="ğŸ“ è¾“å…¥æ–‡å­—æˆ–ä½¿ç”¨éº¦å…‹é£å½•éŸ³...",
                    container=False,
                    scale=4,
                    max_lines=3
                )
                submit_btn = gr.Button("Send", variant='primary', scale=1)

            # âœ… æ–°å¢éº¦å…‹é£è¯­éŸ³è¯†åˆ«æŒ‰é’®
            with gr.Row():
                mic = gr.Audio(
                    sources=["microphone"],
                    type="numpy",
                    label="ğŸ¤ è¯­éŸ³è¾“å…¥ï¼ˆç‚¹å‡»å½•åˆ¶ä¸­æ–‡ï¼‰",
                )
                voice_to_text_btn = gr.Button("ğŸ§ è¯­éŸ³è½¬æ–‡å­—", variant='secondary')

            # å½“ç‚¹å‡»â€œè¯­éŸ³è½¬æ–‡å­—â€æ—¶ï¼Œè°ƒç”¨ Whisper è¯†åˆ«
            voice_to_text_btn.click(
                fn=whisper_recognize,
                inputs=mic,
                outputs=msg,
                show_progress=True
            )

            # ç»§ç»­ä¿æŒåŸå¯¹è¯é€»è¾‘
            with gr.Row():
                clear_btn = gr.Button("Clear", variant='secondary')
                stop_btn = gr.Button("Stop", variant='stop')

            with gr.Accordion("Parameters", open=False):
                with gr.Row():
                    temperature = gr.Slider(0.1, 1.5, 0.6, label="Temperature")
                    top_p = gr.Slider(0.1, 1.0, 0.95, label="Top-p")
                with gr.Row():
                    max_new_tokens = gr.Slider(2048, 32768, 4096, step=64, label="Max Tokens")
                    repetition_penalty = gr.Slider(1, 1.5, 1.2, step=0.01, label="Repetition Penalty")

            gr.Examples(
                examples=[
                    ["æœ€è¿‘å‹åŠ›å¾ˆå¤§ï¼Œæ€»æ˜¯ç¡ä¸å¥½ï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿ"],
                    ["æˆ‘å’Œçˆ¶æ¯æ€»æ˜¯æ²Ÿé€šä¸é¡ºï¼Œä»–ä»¬æ€»è§‰å¾—æˆ‘ä¸æ‡‚äº‹ã€‚"],
                    ["æˆ‘å®³æ€•å¤±è´¥ï¼Œæ€»è§‰å¾—è‡ªå·±ä¸å¤Ÿå¥½ã€‚"],
                    ["æˆ‘å–œæ¬¢ä¸€ä¸ªäººï¼Œä½†ä¸æ•¢è¡¨ç™½ã€‚"],
                    ["æ€ä¹ˆæ‰èƒ½è®©è‡ªå·±æ›´æœ‰è‡ªä¿¡ï¼Ÿ"]
                ],
                inputs=msg,
                label="ğŸ’¬ å’¨è¯¢ç¤ºä¾‹ï¼ˆç‚¹å‡»å¯å¿«é€Ÿå¼€å§‹å¯¹è¯ï¼‰"
            )

            with gr.Row():
                output_audio = gr.Audio(label="Converted Voice", streaming=True, autoplay=True)
                tts_time_display = gr.Textbox(label="TTS Conversion Time", value="0s", interactive=False)

            # === æ¨¡å‹æ¨ç†é€»è¾‘ï¼ˆä½ å·²æœ‰çš„ generate_wrapperï¼‰ ===
            # è¿™é‡Œå‡è®¾ generate_wrapper å·²ç»å®šä¹‰
            text_to_audio_mappings = load_text_audio_mappings(audio_path, slicer_list)
            default_audio_select = list(text_to_audio_mappings.keys())[0] if text_to_audio_mappings else ""
            default_ref_text = default_audio_select
            default_prompt_language = "zh"
            default_text_language = "zh"
            default_how_to_cut = "æŒ‰æ ‡ç‚¹ç¬¦å·åˆ‡"

            submit_event = submit_btn.click(
                user, [msg, chatbot], [msg, chatbot], queue=False
            ).then(
                lambda: [True], outputs=active_gen
            ).then(
                generate_wrapper,
                [
                    chatbot, temperature, top_p, max_new_tokens, repetition_penalty, active_gen,
                    gr.State(default_audio_select), gr.State(default_ref_text),
                    gr.State(default_prompt_language), gr.State(default_text_language),
                    gr.State(default_how_to_cut)
                ],
                [chatbot, output_audio, tts_time_display]
            )

            stop_btn.click(lambda: [False], None, active_gen, cancels=[submit_event])
            clear_btn.click(lambda: (None, None, "0s"), None,
                            [chatbot, output_audio, tts_time_display],
                            queue=False).then(lambda: [False], None, active_gen,
                                              cancels=[submit_event])

    return demo



if __name__ == "__main__":
    logger.info("Loading Deepseek-R1 model...")

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    tokenizer.use_default_system_prompt = False  # å…³é—­ tokenizer è‡ªåŠ¨åœ¨å¯¹è¯æœ€å‰é¢è¿½åŠ ã€Œç³»ç»Ÿé»˜è®¤æç¤ºè¯ã€çš„è¡Œä¸º

    app = build_app()
    app.queue(api_open=True, max_size=20, default_concurrency_limit=20).launch(server_name="0.0.0.0", server_port=7860,
                                                                               max_threads=40)
